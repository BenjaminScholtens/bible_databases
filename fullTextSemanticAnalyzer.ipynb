{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/benjaminscholtens/Library/Python/3.11/lib/python/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_asv\n",
      "t_bbe\n",
      "t_kjv\n",
      "t_web\n",
      "t_ylt\n",
      "[{'field': [1001001, 1, 1, 1, \"In the beginning of God's preparing the heavens and the earth --\"]}]\n"
     ]
    }
   ],
   "source": [
    "# Function to get embeddings\n",
    "def get_embeddings(text):\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    # Use the mean of the last hidden layer's output as the embedding\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Function to load data from json file and append to a list\n",
    "def load_data(filename):\n",
    "    # print('load_data was called')\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Return only the first 3 verses\n",
    "    # return data[\"resultset\"][\"row\"][:100]\n",
    "    return data[\"resultset\"][\"row\"]\n",
    "    \n",
    "# Load data from each file\n",
    "files = ['./json/t_asv.json', './json/t_bbe.json', './json/t_kjv.json', './json/t_web.json', './json/t_ylt.json']\n",
    "data = {}\n",
    "for file in files:\n",
    "    version_code = os.path.splitext(os.path.basename(file))[0]\n",
    "    print(version_code)\n",
    "    data[version_code] = load_data(file)\n",
    "    \n",
    "print(data[version_code][0:1])\n",
    "# Initialize list for storing verse objects\n",
    "set_of_verse_versions = []\n",
    "\n",
    "# Build verse objects\n",
    "for uid in data['t_asv']:\n",
    "    verse_uid = uid[\"field\"][0]\n",
    "    book = uid[\"field\"][1]\n",
    "    chapter = uid[\"field\"][2]\n",
    "    verse_number = uid[\"field\"][3]\n",
    "    # Skip verses greater than 20\n",
    "    # if verse_number > 100:\n",
    "    #     continue\n",
    "    for version_code, verses in data.items():\n",
    "        for verse in verses:\n",
    "            if verse[\"field\"][0] == verse_uid:\n",
    "                # print(verse_uid, version_code)\n",
    "                verse_text = verse[\"field\"][4]\n",
    "                set_of_verse_versions.append({\"verse_uid\": verse_uid, \"book\": book, \"chapter\": chapter, \"verse_number\": verse_number, \"verse_text\": verse_text, \"version_code\": version_code})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate cosine similarity\n",
    "resultsFilename = 'cosine_similarity_results.tsv'\n",
    "logFilename = 'processed_verses.log'\n",
    "header_needed = not (os.path.isfile(resultsFilename) and os.path.getsize(resultsFilename) > 0)\n",
    "\n",
    "# Check if the log file exists, if not create it\n",
    "if not os.path.exists(logFilename):\n",
    "    with open(logFilename, 'w') as log_file:\n",
    "        pass\n",
    "\n",
    "with open(resultsFilename, 'a') as file, open(logFilename, 'r') as log_file:\n",
    "    writer = csv.writer(file, delimiter='\\t')\n",
    "    \n",
    "    if header_needed:\n",
    "        writer.writerow([\"verse_uid\", \"version_code_1\", \"version_code_2\", \"cosine_similarity\"]) # writing headers\n",
    "\n",
    "    processed_verses = log_file.read().splitlines() # Get all processed verses\n",
    "\n",
    "    cosine_sim_results = []\n",
    "    for i in range(len(set_of_verse_versions)):\n",
    "        for j in range(i+1, len(set_of_verse_versions)):\n",
    "            if set_of_verse_versions[i]['verse_uid'] == set_of_verse_versions[j]['verse_uid'] and str(set_of_verse_versions[i]['verse_uid']) not in processed_verses:\n",
    "                emb_i = get_embeddings(set_of_verse_versions[i]['verse_text'])\n",
    "                emb_j = get_embeddings(set_of_verse_versions[j]['verse_text'])\n",
    "                sim = cosine_similarity(emb_i, emb_j)\n",
    "                cosine_sim_results.append({\"verse_uid\": set_of_verse_versions[i]['verse_uid'], \"version_code_1\": set_of_verse_versions[i]['version_code'], \"version_code_2\": set_of_verse_versions[j]['version_code'], \"cosine_similarity\": sim[0][0]})\n",
    "\n",
    "                if len(cosine_sim_results) >= 10: # Write every 10 results\n",
    "                    for result in cosine_sim_results:\n",
    "                        writer.writerow([result[\"verse_uid\"], result[\"version_code_1\"], result[\"version_code_2\"], result[\"cosine_similarity\"]])\n",
    "                    cosine_sim_results = [] # Reset the results\n",
    "\n",
    "                    # Save the processed verse_uid\n",
    "                    with open(logFilename, 'a') as log_file:\n",
    "                        log_file.write(str(set_of_verse_versions[i]['verse_uid']) + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Write remaining results to file\n",
    "if cosine_sim_results: # Check if there are any remaining results that haven't been written to file\n",
    "    with open(resultsFilename, 'a') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t')\n",
    "        writer.writerows(cosine_sim_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/wttzlx4x1xg_lk2q001mzkqr0000gn/T/ipykernel_16616/1867589963.py:6: DtypeWarning: Columns (0,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('cosine_similarity_results.tsv', sep='\\t')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m required_uids \u001b[39m=\u001b[39m counts[counts \u001b[39m==\u001b[39m \u001b[39m10\u001b[39m]\u001b[39m.\u001b[39mindex\n\u001b[1;32m     14\u001b[0m \u001b[39m# Sort 'verse_uid' in descending order\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m sorted_uids \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39;49m(required_uids, reverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Save these 'verse_uid' to a log file\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mprocessed_verses.log\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "# If log file is corrupted, this script can be used to generate a log file of verses with exactly 10 instances\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read your tsv file into a pandas dataframe\n",
    "df = pd.read_csv('cosine_similarity_results.tsv', sep='\\t')\n",
    "\n",
    "# Get the counts of each unique 'verse_uid'\n",
    "counts = df['verse_uid'].value_counts()\n",
    "\n",
    "# Filter those 'verse_uid' with exactly 10 instances\n",
    "required_uids = counts[counts == 10].index\n",
    "\n",
    "# Sort 'verse_uid' in descending order\n",
    "sorted_uids = sorted(required_uids, reverse=False)\n",
    "\n",
    "# Save these 'verse_uid' to a log file\n",
    "with open('processed_verses.log', 'w') as f:\n",
    "    for uid in sorted_uids:\n",
    "        f.write(f'{uid}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
